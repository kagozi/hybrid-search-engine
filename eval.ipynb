{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8efec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval/beir_eval.py\n",
    "import json\n",
    "import psycopg2\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from ranx import Qrels, Run, evaluate\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "# Add project root to path\n",
    "# sys.path.insert(0, str(pathlib.Path(__file__).resolve().parents[1]))\n",
    "from fusion.adaptive_fusion import get_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8c14996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# CONFIGURATION\n",
    "# ================================\n",
    "DATASETS = [\"scifact\", \"trec-covid\"]\n",
    "TOP_K = 100\n",
    "RERANK_K = 30\n",
    "FINAL_K = 10\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    port=5433,\n",
    "    dbname=\"ir_db\",\n",
    "    user=\"postgres\",\n",
    "    password=\"mysecretpassword\"\n",
    ")\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93ea44b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "dense_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efdf0ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map dataset names to table names\n",
    "def get_table_name(dataset_name):\n",
    "    return f\"beir_{dataset_name.replace('-', '_')}\"\n",
    "\n",
    "def download_beir_dataset(dataset_name):\n",
    "    \"\"\"Download BEIR dataset metadata (queries and qrels only)\"\"\"\n",
    "    dataset_path = f\"data/beir/{dataset_name}\"\n",
    "    if os.path.exists(dataset_path):\n",
    "        print(f\"Dataset {dataset_name} already exists, skipping download.\")\n",
    "        return dataset_path\n",
    "    \n",
    "    print(f\"Downloading {dataset_name}...\")\n",
    "    url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset_name}.zip\"\n",
    "    \n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    r = requests.get(url, timeout=120, headers=headers, stream=True)\n",
    "    r.raise_for_status()\n",
    "    \n",
    "    os.makedirs(\"data/beir\", exist_ok=True)\n",
    "    zip_path = f\"data/beir/{dataset_name}.zip\"\n",
    "    \n",
    "    with open(zip_path, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    \n",
    "    print(f\"Extracting...\")\n",
    "    with zipfile.ZipFile(zip_path) as z:\n",
    "        z.extractall(\"data/beir\")\n",
    "    \n",
    "    os.remove(zip_path)\n",
    "    return dataset_path\n",
    "\n",
    "def bm25_search(query, table_name, limit=TOP_K):\n",
    "    \"\"\"BM25 search on specified table\"\"\"\n",
    "    cur.execute(f\"\"\"\n",
    "        SELECT id, ts_rank_cd(clean_text, plainto_tsquery(%s)) AS score\n",
    "        FROM {table_name}\n",
    "        WHERE clean_text @@ plainto_tsquery(%s)\n",
    "        ORDER BY score DESC LIMIT %s\n",
    "    \"\"\", (query, query, limit))\n",
    "    return [(row[0], float(row[1])) for row in cur.fetchall()]\n",
    "\n",
    "def dense_search(query, table_name, limit=TOP_K):\n",
    "    \"\"\"Dense search on specified table\"\"\"\n",
    "    q_emb = dense_model.encode(query, normalize_embeddings=True)\n",
    "    cur.execute(f\"\"\"\n",
    "        SELECT id, embedding <=> %s::vector AS dist\n",
    "        FROM {table_name}\n",
    "        ORDER BY dist LIMIT %s\n",
    "    \"\"\", (q_emb.tolist(), limit))\n",
    "    results = cur.fetchall()\n",
    "    \n",
    "    if not results:\n",
    "        return []\n",
    "    \n",
    "    scores = [1 - row[1] for row in results]\n",
    "    max_score = max(scores) if scores else 1\n",
    "    normalized = [s / max_score for s in scores]\n",
    "    return [(row[0], normalized[i]) for i, row in enumerate(results)]\n",
    "\n",
    "def get_document_text(doc_id, table_name):\n",
    "    \"\"\"Retrieve document text for reranking\"\"\"\n",
    "    cur.execute(f\"SELECT text FROM {table_name} WHERE id = %s\", (doc_id,))\n",
    "    row = cur.fetchone()\n",
    "    return row[0] if row else \"\"\n",
    "\n",
    "def rerank_documents(query, doc_ids, table_name):\n",
    "    \"\"\"Rerank documents using cross-encoder\"\"\"\n",
    "    passages = [get_document_text(doc_id, table_name)[:1000] for doc_id in doc_ids]\n",
    "    pairs = [[query, passage] for passage in passages if passage]\n",
    "    \n",
    "    if not pairs:\n",
    "        return []\n",
    "    \n",
    "    # Get raw logits from cross-encoder\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    # Convert logits to positive scores using sigmoid\n",
    "    # This ensures scores are between 0 and 1\n",
    "    import math\n",
    "    positive_scores = [1 / (1 + math.exp(-score)) for score in scores]\n",
    "    \n",
    "    return positive_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd267cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "=== Evaluating on SCIFACT ===\n",
      "============================================================\n",
      "✓ Using table: beir_scifact (5,183 documents)\n",
      "Dataset scifact already exists, skipping download.\n",
      "Loaded 1109 queries. Sample query IDs: ['0', '2', '4', '6', '9']\n",
      "Loaded 300 qrels. Sample qrel query IDs: ['1', '3', '5', '13', '36']\n",
      "Sample qid: 1\n",
      "Sample qrel docids for that qid: ['31715818']\n",
      "31715818 exists in DB? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running queries:   0%|          | 0/1109 [00:00<?, ?it/s]/Users/alexkagozi/Desktop/AI/ArtificialIntelligence/hybrid-search-engine/.hybrid-engine/lib/python3.9/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "Running queries: 100%|██████████| 1109/1109 [37:12<00:00,  2.01s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating SCIFACT...\n",
      "\n",
      "Results on SCIFACT:\n",
      "------------------------------------------------------------\n",
      "BM25:\n",
      "  ndcg@10: 0.0436\n",
      "  recall@100: 0.0428\n",
      "  map@100: 0.0428\n",
      "Dense (MiniLM):\n",
      "  ndcg@10: 0.6424\n",
      "  recall@100: 0.8753\n",
      "  map@100: 0.5998\n",
      "RRF:\n",
      "  ndcg@10: 0.6548\n",
      "  recall@100: 0.8753\n",
      "  map@100: 0.6140\n",
      "Adaptive Fusion + Re-rank (Yours):\n",
      "  ndcg@10: 0.6684\n",
      "  recall@100: 0.8140\n",
      "  map@100: 0.6170\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "=== Evaluating on TREC-COVID ===\n",
      "============================================================\n",
      "✓ Using table: beir_trec_covid (171,332 documents)\n",
      "Dataset trec-covid already exists, skipping download.\n",
      "Loaded 50 queries. Sample query IDs: ['1', '2', '3', '4', '5']\n",
      "Loaded 50 qrels. Sample qrel query IDs: ['1', '2', '3', '4', '5']\n",
      "Sample qid: 1\n",
      "Sample qrel docids for that qid: ['005b2j4b', '00fmeepz', 'g7dhmyyo', '0194oljo', '021q9884', '02f0opkr', '047xpt2c', '04ftw7k9', 'pl9ht0d0', '05vx82oo']\n",
      "005b2j4b exists in DB? True\n",
      "00fmeepz exists in DB? True\n",
      "g7dhmyyo exists in DB? True\n",
      "0194oljo exists in DB? True\n",
      "021q9884 exists in DB? True\n",
      "02f0opkr exists in DB? True\n",
      "047xpt2c exists in DB? True\n",
      "04ftw7k9 exists in DB? True\n",
      "pl9ht0d0 exists in DB? True\n",
      "05vx82oo exists in DB? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running queries:   0%|          | 0/50 [00:00<?, ?it/s]/Users/alexkagozi/Desktop/AI/ArtificialIntelligence/hybrid-search-engine/.hybrid-engine/lib/python3.9/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "Running queries: 100%|██████████| 50/50 [00:50<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating TREC-COVID...\n",
      "\n",
      "Results on TREC-COVID:\n",
      "------------------------------------------------------------\n",
      "BM25:\n",
      "  ndcg@10: 0.1986\n",
      "  recall@100: 0.0208\n",
      "  map@100: 0.0135\n",
      "Dense (MiniLM):\n",
      "  ndcg@10: 0.4661\n",
      "  recall@100: 0.0412\n",
      "  map@100: 0.0285\n",
      "RRF:\n",
      "  ndcg@10: 0.5003\n",
      "  recall@100: 0.0565\n",
      "  map@100: 0.0376\n",
      "Adaptive Fusion + Re-rank (Yours):\n",
      "  ndcg@10: 0.3549\n",
      "  recall@100: 0.0088\n",
      "  map@100: 0.0073\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "=== SUMMARY TABLE FOR IEEE REPORT ===\n",
      "============================================================\n",
      "| Method                        | SciFact nDCG@10 | TREC-COVID nDCG@10 |\n",
      "|-------------------------------|-----------------|--------------------|\n",
      "| BM25                          |          0.0436 |             0.1986 |\n",
      "| Dense (MiniLM)                |          0.6424 |             0.4661 |\n",
      "| RRF                           |          0.6548 |             0.5003 |\n",
      "| Adaptive Fusion + Re-rank (Yours) |          0.6684 |             0.3549 |\n",
      "\n",
      "Evaluation complete! Results saved in eval/output/\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(dataset_name):\n",
    "    \"\"\"Load queries and qrels\"\"\"\n",
    "    path = download_beir_dataset(dataset_name)\n",
    "    \n",
    "    queries_path = f\"{path}/queries.jsonl\"\n",
    "    with open(queries_path) as f:\n",
    "        # CRITICAL: Keep query IDs as strings exactly as they appear\n",
    "        queries = {}\n",
    "        for line in f:\n",
    "            q = json.loads(line)\n",
    "            qid = str(q['_id'])  # Ensure string\n",
    "            queries[qid] = q['text']\n",
    "    \n",
    "    print(f\"Loaded {len(queries)} queries. Sample query IDs: {list(queries.keys())[:5]}\")\n",
    "    \n",
    "    # qrels_path = f\"{path}/qrels/test.tsv\"\n",
    "    # with open(qrels_path) as f:\n",
    "    #     qrels_dict = {}\n",
    "    #     for i, line in enumerate(f):\n",
    "    #         if i == 0 and 'query-id' in line.lower():\n",
    "    #             continue\n",
    "            \n",
    "    #         parts = line.strip().split('\\t')\n",
    "    #         if len(parts) >= 3:\n",
    "    #             qid = str(parts[0])  # Ensure string\n",
    "    #             docid = str(parts[2])  # Ensure string\n",
    "    #             rel = int(parts[3]) if len(parts) > 3 else 1\n",
    "    #             qrels_dict.setdefault(qid, {})[docid] = rel\n",
    "    qrels_path = f\"{path}/qrels/test.tsv\"\n",
    "    with open(qrels_path) as f:\n",
    "        qrels_dict = {}\n",
    "        for i, line in enumerate(f):\n",
    "            # Skip header if present\n",
    "            if i == 0 and any(h in line.lower() for h in [\"query-id\", \"qid\", \"topic\"]):\n",
    "                continue\n",
    "\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "\n",
    "            if len(parts) == 3:\n",
    "                # BEIR-style: query-id corpus-id score\n",
    "                qid, docid, rel = parts\n",
    "            else:\n",
    "                # TREC-style: query-id Q0 docid rel [iteration...]\n",
    "                qid, _q0, docid, rel = parts[:4]\n",
    "\n",
    "            qid = str(qid)\n",
    "            docid = str(docid)\n",
    "            rel = int(rel)\n",
    "\n",
    "            qrels_dict.setdefault(qid, {})[docid] = rel\n",
    "\n",
    "    \n",
    "    print(f\"Loaded {len(qrels_dict)} qrels. Sample qrel query IDs: {list(qrels_dict.keys())[:5]}\")\n",
    "    \n",
    "    return queries, qrels_dict\n",
    "\n",
    "def rrf_fusion(bm25_results, dense_results, k=60):\n",
    "    \"\"\"Reciprocal Rank Fusion\"\"\"\n",
    "    scores = {}\n",
    "    for i, (doc_id, _) in enumerate(bm25_results):\n",
    "        scores[str(doc_id)] = scores.get(str(doc_id), 0) + 1 / (i + k)\n",
    "    for i, (doc_id, _) in enumerate(dense_results):\n",
    "        scores[str(doc_id)] = scores.get(str(doc_id), 0) + 1 / (i + k)\n",
    "    return scores\n",
    "\n",
    "# ================================\n",
    "# EVALUATION LOOP\n",
    "# ================================\n",
    "results_summary = {}\n",
    "\n",
    "for dataset_name in DATASETS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"=== Evaluating on {dataset_name.upper()} ===\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    table_name = get_table_name(dataset_name)\n",
    "    \n",
    "    # Check if table exists\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT EXISTS (\n",
    "            SELECT FROM information_schema.tables \n",
    "            WHERE table_schema = 'public' \n",
    "            AND table_name = %s\n",
    "        );\n",
    "    \"\"\", (table_name,))\n",
    "    \n",
    "    if not cur.fetchone()[0]:\n",
    "        print(f\"❌ Table {table_name} does not exist!\")\n",
    "        print(f\"Please run: python indexing/index_beir.py\")\n",
    "        continue\n",
    "    \n",
    "    # Verify table has documents\n",
    "    cur.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "    doc_count = cur.fetchone()[0]\n",
    "    print(f\"✓ Using table: {table_name} ({doc_count:,} documents)\")\n",
    "    \n",
    "    try:\n",
    "        queries, qrels_dict = load_dataset(dataset_name)\n",
    "        qrels = Qrels(qrels_dict)\n",
    "        some_qid = next(iter(qrels_dict.keys()))\n",
    "        print(\"Sample qid:\", some_qid)\n",
    "        print(\"Sample qrel docids for that qid:\", list(qrels_dict[some_qid].keys())[:10])\n",
    "\n",
    "        table_name = get_table_name(dataset_name)\n",
    "        for did in list(qrels_dict[some_qid].keys())[:10]:\n",
    "            cur.execute(f\"SELECT COUNT(*) FROM {table_name} WHERE id = %s\", (did,))\n",
    "            count = cur.fetchone()[0]\n",
    "            print(did, \"exists in DB?\" , count > 0)\n",
    "\n",
    "        run_bm25 = {}\n",
    "        run_dense = {}\n",
    "        run_rrf = {}\n",
    "        run_adaptive = {}\n",
    "        \n",
    "        for qid, query in tqdm(queries.items(), desc=\"Running queries\"):\n",
    "            try:\n",
    "                # 1. BM25\n",
    "                bm25_results = bm25_search(query, table_name, TOP_K)\n",
    "                run_bm25[qid] = {str(doc_id): score for doc_id, score in bm25_results} if bm25_results else {}\n",
    "                \n",
    "                # 2. Dense\n",
    "                dense_results = dense_search(query, table_name, TOP_K)\n",
    "                run_dense[qid] = {str(doc_id): score for doc_id, score in dense_results} if dense_results else {}\n",
    "                \n",
    "                # 3. RRF\n",
    "                rrf_scores = rrf_fusion(bm25_results, dense_results)\n",
    "                run_rrf[qid] = rrf_scores if rrf_scores else {}\n",
    "                \n",
    "                # 4. Adaptive Fusion + Re-rank\n",
    "                alpha = get_alpha(query)\n",
    "                fused = {}\n",
    "                bm25_dict = {doc_id: score for doc_id, score in bm25_results}\n",
    "                dense_dict = {doc_id: score for doc_id, score in dense_results}\n",
    "                all_ids = set(bm25_dict) | set(dense_dict)\n",
    "                \n",
    "                for doc_id in all_ids:\n",
    "                    s_bm25 = bm25_dict.get(doc_id, 0)\n",
    "                    s_dense = dense_dict.get(doc_id, 0)\n",
    "                    fused[doc_id] = alpha * s_bm25 + (1 - alpha) * s_dense\n",
    "                \n",
    "                candidates = sorted(fused.items(), key=lambda x: x[1], reverse=True)[:RERANK_K]\n",
    "                candidate_ids = [doc_id for doc_id, _ in candidates]\n",
    "                \n",
    "                if candidate_ids:\n",
    "                    rerank_scores = rerank_documents(query, candidate_ids, table_name)\n",
    "                    final_scored = sorted(zip(candidate_ids, rerank_scores), key=lambda x: x[1], reverse=True)[:FINAL_K]\n",
    "                    run_adaptive[qid] = {str(doc_id): score for doc_id, score in final_scored}\n",
    "                else:\n",
    "                    run_adaptive[qid] = {}\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing query {qid}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save runs\n",
    "        os.makedirs(\"eval/output\", exist_ok=True)\n",
    "        Run(run_bm25).save(f\"eval/output/{dataset_name}_bm25.json\")\n",
    "        Run(run_dense).save(f\"eval/output/{dataset_name}_dense.json\")\n",
    "        Run(run_rrf).save(f\"eval/output/{dataset_name}_rrf.json\")\n",
    "        Run(run_adaptive).save(f\"eval/output/{dataset_name}_adaptive.json\")\n",
    "        \n",
    "        # Evaluate\n",
    "        print(f\"\\nEvaluating {dataset_name.upper()}...\")\n",
    "        metrics = [\"ndcg@10\", \"recall@100\", \"map@100\"]\n",
    "        eval_results = {}\n",
    "        \n",
    "        bm25_run = Run(run_bm25)\n",
    "        bm25_scores = evaluate(qrels, bm25_run, metrics, make_comparable=True)\n",
    "        eval_results[\"BM25\"] = bm25_scores\n",
    "        \n",
    "        dense_run = Run(run_dense)\n",
    "        dense_scores = evaluate(qrels, dense_run, metrics, make_comparable=True)\n",
    "        eval_results[\"Dense (MiniLM)\"] = dense_scores\n",
    "        \n",
    "        rrf_run = Run(run_rrf)\n",
    "        rrf_scores = evaluate(qrels, rrf_run, metrics, make_comparable=True)\n",
    "        eval_results[\"RRF\"] = rrf_scores\n",
    "        \n",
    "        adaptive_run = Run(run_adaptive)\n",
    "        adaptive_scores = evaluate(qrels, adaptive_run, metrics, make_comparable=True)\n",
    "        eval_results[\"Adaptive Fusion + Re-rank (Yours)\"] = adaptive_scores\n",
    "        \n",
    "        results_summary[dataset_name] = eval_results\n",
    "        \n",
    "        print(f\"\\nResults on {dataset_name.upper()}:\")\n",
    "        print(\"-\" * 60)\n",
    "        for method, scores in eval_results.items():\n",
    "            print(f\"{method}:\")\n",
    "            for metric, value in scores.items():\n",
    "                print(f\"  {metric}: {value:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {dataset_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Summary Table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== SUMMARY TABLE FOR IEEE REPORT ===\")\n",
    "print(\"=\"*60)\n",
    "print(\"| Method                        | SciFact nDCG@10 | TREC-COVID nDCG@10 |\")\n",
    "print(\"|-------------------------------|-----------------|--------------------|\")\n",
    "for method in [\"BM25\", \"Dense (MiniLM)\", \"RRF\", \"Adaptive Fusion + Re-rank (Yours)\"]:\n",
    "    sci = results_summary.get(\"scifact\", {}).get(method, {}).get(\"ndcg@10\", \"N/A\")\n",
    "    trec = results_summary.get(\"trec-covid\", {}).get(method, {}).get(\"ndcg@10\", \"N/A\")\n",
    "    if isinstance(sci, float) and isinstance(trec, float):\n",
    "        print(f\"| {method:<29} | {sci:>15.4f} | {trec:>18.4f} |\")\n",
    "    else:\n",
    "        print(f\"| {method:<29} | {str(sci):>15} | {str(trec):>18} |\")\n",
    "\n",
    "print(\"\\nEvaluation complete! Results saved in eval/output/\")\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce4226d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b03e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".hybrid-engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
